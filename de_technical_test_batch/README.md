# Project 1: Batch ELT Pipeline (Task 1 & 2)

This pipeline uses Airflow, Postgres, and Docker to execute an ELT workflow:
1.  **Extract:** 5 CSV files are generated by the `data_generator`.
2.  **Load:** The `task_1_load_to_staging` DAG loads raw data into the `staging` schema.
3.  **Transform:** The `task_2_transform_to_dwh` DAG transforms data from `staging` into the final `dwh` Star Schema.

**Technology Stack:** Docker, Apache Airflow (LocalExecutor), PostgreSQL.

---

## Task 2: Deliverables (Data Modeling)

### 1. ERD (Entity-Relationship Diagram)
The following Star Schema is used for the Data Warehouse (DWH):

![ERD Diagram](../assets/ERD%20Task%202%20New.png)

### 2. Explanation of Modeling Choices
* **Star Schema:** I chose a Star Schema as it is the industry standard for analytical DWH design. It simplifies queries (minimizing JOINs) and is highly optimized for aggregation by BI tools.
* **ELT (Extract-Load-Transform):** A modern ELT approach was used. Raw data is **L**oaded directly into the `staging` schema. The **T**ransformation (JOINs, cleaning, key lookups) occurs *within* the database, which is far more scalable than in-memory transformations.
* **Surrogate Keys:** All `dim_` tables use a `GENERATED ... AS IDENTITY` surrogate key (e.g., `customer_key`). This decouples the DWH from operational keys (`customer_id_natural`) and allows for future Slowly Changing Dimensions (SCD) management.
* **`dwh.dim_date`:** This dimension table is programmatically generated. This is a DWH best practice that allows analysts to query facts by any time attribute (day, month, quarter) without complex date functions at query time.

### 3. SQL DDL
* All DDL scripts (for both `staging` and `dwh` schemas) are located in the `sql/` folder.
* These are **automatically executed** by the Postgres container's `docker-entrypoint-initdb.d` service on the first `docker-compose up`.

---

## ðŸš€ Setup & Run Instructions

Please follow these steps sequentially.

### 1. Initial Setup (One-Time Only)
(Only required for a clean-slate run)

```bash
# 1. Clean up old data/networks (CRITICAL: use -v (lowercase) to delete old volumes)
docker-compose down -v

# 2. Create .env file (for Windows permissions)
echo AIRFLOW_UID=50000 > .env

# 3. Create required Docker folders
mkdir -p ./logs ./plugins
```
### 2\. Run the Environment (Airflow + Postgres)

This command will **automatically create all database tables** (staging.\* and dwh.\*) using the scripts in the sql/ folder.

Bash

```docker-compose up -d```

> _(Wait 2-3 minutes for all services to become (healthy))_

### 3\. Configure Airflow (Manual - One-Time Only)

1.  Bash
```docker-compose exec airflow-webserver airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com```
    
2.  **Login to http://localhost:8080** (user: ```admin```, pass: ```admin```).
    
3.  **Create Connection:** Go to **Admin > Connections > Add** and fill in:
    
    *   **Conn Id:** ```postgres_staging_db```
        
    *   **Conn Type:** ```Postgres```
        
    *   **Host:** ```postgres```
        
    *   **Login:** ```airflow```
        
    *   **Password:** ```airflow```
        
    *   **Port:** ```5432```
        

### 4\. Run the Pipeline

1.  Bash # (Create and activate a local venv)# 
```pip install -r requirements.txt``` 
    
2.  **Trigger DAG 1 (Load):** In the Airflow UI, un-pause and trigger the ```task_1_load_to_staging``` DAG.
    
3.  **Trigger DAG 2 (Transform):** After DAG 1 succeeds, un-pause and trigger the ```task_2_transform_to_dwh``` DAG.
    
### 5\. Verification

You can verify the final result in DBeaver (or any DB client).

**Check Staging (After DAG 1):**

SQL

` SELECT COUNT(*) FROM staging.stg_customers;`  -- Expected: 1000 

**Check DWH (After DAG 2):**

SQL

` SELECT COUNT(*) FROM dwh.dim_customer; `  -- Expected: 1001  
`SELECT COUNT(*) FROM dwh.dim_date;`  -- Expected: > 0 (e.g., ~4018)  
`SELECT COUNT(*) FROM dwh.fact_sales;`  -- Expected: > 0 (e.g., ~5214)

**Check Idempotency (Run DAG 2 a second time):**
SQL

`SELECT COUNT(*) FROM dwh.fact_sales;`-- Expected: Count MUST remain the same (e.g., 5214)

**Example Output Verification Check:**

![Batch Verification Output](../assets/Output%20Task%202.png)

### 6\. To Shut Down

Bash

` docker-compose down `
