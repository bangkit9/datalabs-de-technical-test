# Datalabs Data Engineering Technical Test Submission #
---
This repository contains the complete solution for the **Datalabs Data Engineering Technical Test**. The project is designed to evaluate the ability to build, model, and explain data pipelines.
To demonstrate proficiency across different data paradigmsâ€”and to manage hardware constraints effectively (tested on 8GB RAM)â€”the solution is bifurcated into two distinct, self-contained projects:

1.  **`/de_technical_test_batch`**
    A complete, containerized **Batch ELT Pipeline** (Task 1, 2, 3) built with Apache Airflow, PostgreSQL, and Docker.

2.  **`/de_technical_test_optional_task`**
    An optional, real-time **Streaming Pipeline** (Task 4) built with Kafka, PySpark, and Docker.
---

## ðŸ›ï¸ Solution Architecture

### 1. Batch ELT Pipeline Architecture (Task 1-2)
This project fulfills both Task 1 (ETL Process) and Task 2 (Data Modeling) by implementing a modern **ELT (Extract-Load-Transform)** architecture. The entire pipeline is containerized with Docker and orchestrated by Apache Airflow.
![Batch ELT Pipeline Architecture](assets/Task%201%262%20Fix.jpg)

### Task 1: The ELT Process
The pipeline is broken into two distinct Airflow DAGs (Directed Acyclic Graphs) that handle the "E-L" and "T" steps separately.
1.  **Extract & Load (DAG 1: `load_staging_data.py`)**
    * **Extract:** The process begins by reading the 5 source tables (`customers`, `products`, etc.) from the local filesystem. This "Sample Data" is generated by the `data_generator/generate_source_data.py` script as required.
    * **Load:** The raw, unaltered data is immediately loaded into a dedicated `staging` schema in the PostgreSQL database (e.g., into `staging.stg_customers`). This approach is highly scalable as it uses the database's optimized bulk-loading capabilities and avoids complex in-memory transformations.

2.  **Transform (DAG 2: `transform_to_dwh.py`)**
    * **Transform:** Once the data is in the `staging` schema, this second DAG executes. It runs a series of SQL scripts *inside* the PostgreSQL database.
    * This SQL logic reads from the raw `staging` tables, performs all necessary joins (e.g., `transactions + transaction_items`), cleans data, and loads the final, modeled data into the `dwh` schema. This fulfills the `User python and airflow to create the pipeline` requirement.

### Task 2: The Data Modeling
The end-goal of the pipeline is a professional Data Warehouse (DWH) designed for analytics.
#### 1. ERD (Entity-Relationship Diagram)
The DWH is designed as a **Star Schema**, which is the industry-standard for analytical workloads. It features one central Fact Table surrounded by four conformed Dimension Tables.

![DWH Star Schema](assets/ERD%20Task%202%20New.png)

#### Explanation of Modeling Choices

* **Star Schema:** I chose a Star Schema for its simplicity and high-performance query capabilities. It allows BI tools (like Tableau or Power BI) to easily join dimensions to facts with minimal complexity, leading to faster dashboards and reports.

* **Fact Table (`fact_sales`):** As required, this table is derived by joining `transactions` and `transaction_items`. The **granularity** (level of detail) is one row per *transaction item*. This is crucial as it allows analysts to aggregate metrics (like `total_amount`) by any dimension (e.g., by Product, by Customer, or by Date).

* **Dimension Tables (`dim_*`):**
    * `dim_customer`, `dim_product`, `dim_campaign`: These tables provide the "who, what, and why" context for the `fact_sales` table.
    * `dim_date`: This is a programmatically generated dimension table. It is a DWH best practice that allows analysts to query data by attributes like `day_of_week`, `month_name`, or `quarter_of_year` without performing heavy date calculations during the query.

* **Surrogate Keys:** All dimension tables use a `GENERATED ALWAYS AS IDENTITY` column (e.g., `customer_key`) as their Primary Key. This is a critical modeling choice that:
    1.  **Decouples** the DWH from the source system's keys (`customer_id_natural`).
    2.  Allows the DWH to handle **Slowly Changing Dimensions (SCD)** in the future (e.g., tracking a customer's change of address) without breaking historical facts.
    
### 2. SQL DDL
* All DDL scripts (for both `staging` and `dwh` schemas) are located in the `sql/` folder.
* These are **automatically executed** by the Postgres container's `docker-entrypoint-initdb.d` service on the first `docker-compose up`.
---  
    
### 2. Real-time Streaming Architecture (Task 4)
The architecture is designed to precisely meet all requirements of Task 4, using a more advanced IoT sensor data use case.
![Streaming Pipeline Architecture](assets/Task%204%20Optional%20Fix.jpg)

### 1. Source: Python Kafka Producer (`producer.py`)
This fulfills the `Source: ... (Python loop or Kafka producer)` requirement.
* A Python script runs an infinite `while True:` loop, simulating a live IoT data feed.
* It generates one complex JSON object (a "data transaction") per second, containing sensor readings (Watt, Volt, Amphere) and device metadata (City, Device Name).
* This data is serialized to JSON and sent to the `sensor_topic` in the Kafka broker.

### 2. Broker: Apache Kafka (Docker)
* A lightweight, containerized Kafka and Zookeeper stack (via `docker-compose.streaming.yaml`) acts as the central message broker.
* It ingests the high-velocity stream from the producer and buffers it, decoupling the source from the processing.

### 3. Process: PySpark Streaming (`consumer.py`)
This fulfills the `Process: Aggregate transaction count per minute` requirement.
* A PySpark script (running locally) subscribes to the `sensor_topic`.
* It parses the incoming JSON stream and converts the string `date_time` into a proper `event_time` timestamp.
* It applies a 1-minute **Tumbling Window** (`window(col("event_time"), "1 minute")`).
* It performs the required aggregation: **`count(*)`** to get the total number of sensor readings (data transactions) received within that 1-minute window.

### 4. Sink: Console Output
This fulfills the `Sink: Console output...` requirement.
* The aggregated, windowed DataFrame is written directly to the console (`format("console")`).
* The query is triggered once per minute, displaying the final count for each completed window.
---

## ðŸš€ How to Run This Project

**ðŸ–¥ï¸Global Prerequisites:**
* Docker Desktop
* Python 3.10+
* Java (JDK 17) (required by PySpark for Task 4)

**IMPORTANT:** Due to hardware constraints, the two pipelines **cannot** be run concurrently. Please shut down one pipeline (`docker-compose down -v`) before running the other.
---

### Part 1: Running the `de_technical_test_batch` Project (Task 1-3)
> **Please open and follow the complete instructions in the `de_technical_test_batch/README.md` file.**
---

### Part 2: Running the `de_technical_test_optional_task` Project (Task 4)
> **Please open and follow the complete instructions in the `de_technical_test_optional_task/README.md` file.**
